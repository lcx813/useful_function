# -*- coding: utf-8 -*-
"""Class7__Regression_Diagnostics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bn1gFGjcA4coKCEbZ1o38KS489ZQB4Ix

# Visual analytics and diagnostics of model fit for linear regression

In this notebook, we show the basic visual analytics and diagnostic tests that should be run after fitting a linear regression model, for **checking the goodness of fit by verifying the fundamental assumptions of linear regression** - *linearity, independence, constant variance,* and *normality*. In particular, following analytics are shown,

* Pairwise scatterplot of the data matrix
* Correlation matrix and heatmap
* Creating a new dataset of predicting features and their statistical significance (based on p-values)
* Residuals vs. predicting variables plots
* Fitted vs. residuals plot
* Histogram of the normalized residuals
* Q-Q plot of the normalized residuals
* Shapiro-Wilk normality test on the residuals
* Cook's distance plot of the residuals
* Variance inflation factor (VIF) of the predicting features

### Requirements

* Python 3.6+
* `Numpy`
* `Pandas`
* `Seaborn`
* `Statsmodels`
* `Scipy`

### The dataset

We analyze the **[Concrete Compressive Strength Data Set](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)** from UCI ML repository in this notebook.

***Abstract***: Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. Can we predict the strength from other measurement values?

#### Data Set Information:

* Number of instances: 1030 
* Number of Attributes: 9 
* Attribute breakdown	8 quantitative input variables, and 1 quantitative output variable 
* Missing Attribute Values: None

### Import the libraries and read the data in
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.formula.api as sm

# The dataset path may be different in your situation. Please use the correct path
df = pd.read_excel("Concrete_Data.xls")

df.head(10)

#.T transpose
df.describe().T

"""### Taking a peek at the relationship between the predicting variables and the response"""

for c in df.columns[:-1]:
    plt.figure(figsize=(8,5))
    plt.title("{} vs. \nConcrete Compressive Strength".format(c),fontsize=16)
    plt.scatter(x=df[c],y=df['Concrete compressive strength(MPa, megapascals) '],color='orange',edgecolor='k')
    plt.grid(True)
    plt.xlabel(c,fontsize=14)
    plt.ylabel('Concrete compressive strength\n(MPa, megapascals)',fontsize=14)
    plt.show()

"""### Creating a copy with suitable column names for processing with `statsmodels.OLS()`"""

df1 = df.copy()

df1.columns=['Component'+str(i) for i in range(1,8)]+['Age']+['y']

df1.head()

"""### Pairwise scatter plots"""

from seaborn import pairplot

pairplot(df1)

"""### Correlation matrix and heatmap to visually check for [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)"""

corr = df1[:-1].corr()

corr

from statsmodels.graphics.correlation import plot_corr

fig=plot_corr(corr,xnames=corr.columns)

"""### Creating a formula string for using in the `statsmodels.OLS()`"""

formula_str = df1.columns[-1]+' ~ '+'+'.join(df1.columns[:-1])

formula_str

"""### Construct and fit the model. Print summary of the fitted model"""

model=sm.ols(formula=formula_str, data=df1)

fitted = model.fit()

print(fitted.summary())

"""### A new Result dataframe: p-values and statistical significance of the features"""

df_result=pd.DataFrame()

df_result['pvalues']=fitted.pvalues[1:]

df_result['Features']=df.columns[:-1]

df_result.set_index('Features',inplace=True)

def yes_no(b):
    if b:
        return 'Yes'
    else:
        return 'No'

df_result['Statistically significant?']= df_result['pvalues'].apply(yes_no)

df_result

"""#### All the predicting variables are statisticall significant with the [threshold of p-value <0.01](https://www.statsdirect.com/help/basics/p_values.htm)

### Residuals vs. predicting variables plots
"""

for c in df.columns[:-1]:
    plt.figure(figsize=(8,5))
    plt.title("{} vs. \nModel residuals".format(c),fontsize=16)
    plt.scatter(x=df[c],y=fitted.resid,color='red',edgecolor='k')
    plt.grid(True)
    xmin=min(df[c])
    xmax = max(df[c])
    plt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='blue',linestyle='--',lw=3)
    plt.xlabel(c,fontsize=14)
    plt.ylabel('Residuals',fontsize=14)
    plt.show()

"""#### Residual plots show some bit of clustering but overall the assumptions linearity and independence seem to hold because the distribution seem random around the 0 axis.

### Fitted vs. residuals
"""

plt.figure(figsize=(8,5))
p=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')
xmin=min(fitted.fittedvalues)
xmax = max(fitted.fittedvalues)
plt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)
plt.xlabel("Fitted values",fontsize=15)
plt.ylabel("Residuals",fontsize=15)
plt.title("Fitted vs. residuals plot",fontsize=18)
plt.grid(True)
plt.show()

"""#### The fitted vs. residuals plot shows violation of the constant variance assumption - [Heteroscedasticity](http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html).

### Histogram of normalized residuals
"""

plt.figure(figsize=(8,5))
plt.hist(fitted.resid_pearson,bins=20,edgecolor='k')
plt.ylabel('Count',fontsize=15)
plt.xlabel('Normalized residuals',fontsize=15)
plt.title("Histogram of normalized residuals",fontsize=18)
plt.show()

"""### Q-Q plot of the residuals"""

from statsmodels.graphics.gofplots import qqplot

plt.figure(figsize=(8,5))
fig=qqplot(fitted.resid_pearson,line='45',fit='True')
plt.xticks(fontsize=13)
plt.yticks(fontsize=13)
plt.xlabel("Theoretical quantiles",fontsize=15)
plt.ylabel("Sample quantiles",fontsize=15)
plt.title("Q-Q plot of normalized residuals",fontsize=18)
plt.grid(True)
plt.show()

"""#### The Q-Q plot (and the histogram above) shows that the normality assumption is satisfied pretty good

### Normality (Shapiro-Wilk) test of the residuals
"""

from scipy.stats import shapiro

_,p=shapiro(fitted.resid)

if p<0.01:
    print("The residuals seem to come from Gaussian process")
else:
    print("The normality assumption may not hold")

"""### Cook's distance (checking for outliers in residuals)"""

from statsmodels.stats.outliers_influence import OLSInfluence as influence

inf=influence(fitted)

(c, p) = inf.cooks_distance
plt.figure(figsize=(8,5))
plt.title("Cook's distance plot for the residuals",fontsize=16)
plt.stem(np.arange(len(c)), c, markerfmt=",")
plt.grid(True)
plt.show()

"""#### There are few data points with residuals being possible outliers

### Variance inflation factor
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

for i in range(len(df1.columns[:-1])):
    v=vif(np.matrix(df1[:-1]),i)
    print("Variance inflation factor for {}: {}".format(df.columns[i],round(v,2)))

"""#### There are few features with VIF > 10, thereby indicating significant multicollinearity"""

